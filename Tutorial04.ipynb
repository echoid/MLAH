{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg7TfjhhqwEi"
      },
      "source": [
        "# Machine Learning Applications for Health (COMP90089_2022_SM2)\n",
        "# Tutorial 3: Unsupervised Learning using MIMIC-IV data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtlAPZR0qwEl"
      },
      "source": [
        "* We are going to play with two types of Clustering Algorithms: K-Means and DB-Scan\n",
        "* The Python lybrary will be sklearn. To check their documentation, please click [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN). \n",
        "\n",
        "The K-Means was based on [this](https://www.kaggle.com/code/minc33/visualizing-high-dimensional-clusters/notebook)  tutorial. Feel free to read in entirely.\n",
        "\n",
        "The DBSCAN was based on [this](https://medium.com/@tarammullin/dbscan-2788cfce9389) tutorial. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IxCNUK_qwEm"
      },
      "source": [
        "Set up the **environment**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "y0TmtjohqwEm"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "from sklearn.decomposition import PCA #Principal Component Analysis\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "#plotly imports\n",
        "#!pip install plotly==5.10.0\n",
        "import plotly as py\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\"\n",
        "\n",
        "# Access data using Google BigQuery.\n",
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def enable_plotly_in_cell():\n",
        "  import IPython\n",
        "  from plotly.offline import init_notebook_mode\n",
        "  display(IPython.core.display.HTML('''<script src=\"/static/components/requirejs/require.js\"></script>'''))\n",
        "  init_notebook_mode(connected=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AYQovpHfMTP7"
      },
      "outputs": [],
      "source": [
        "# authenticate\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HLtANN_eMWBx"
      },
      "outputs": [],
      "source": [
        "# Set up environment variables\n",
        "project_id = 'CHANGE'\n",
        "if project_id == 'CHANGE-ME':\n",
        "  raise ValueError('You must change project_id to your GCP project.')\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = project_id\n",
        "\n",
        "# Read data from BigQuery into pandas dataframes.\n",
        "def run_query(query, project_id=project_id):\n",
        "  return pd.io.gbq.read_gbq(\n",
        "      query,\n",
        "      project_id=project_id,\n",
        "      dialect='standard')\n",
        "\n",
        "# set the dataset\n",
        "# if you want to use the demo, change this to mimic_demo\n",
        "dataset = 'mimiciv'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJII3uxZqwEo"
      },
      "source": [
        "## Data \n",
        "\n",
        "\n",
        "We'll use a cohort derived from MIMIC-IV.\n",
        "\n",
        "* Use the query bellow to search the data in the **BigQuery Platform**.\n",
        "* We are retrieving patients with **Sepsis**: A life-threatening complication caused by the body's response to an infection. When your immune system goes into **overdrive in response to an infection**, sepsis may develop as a result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77HrSabnqwEo",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "##We are retrieving patients using sepsis3 Table and joining it to patients Table.\n",
        "\n",
        "df = run_query(\"\"\"\n",
        "SELECT sep.subject_id,sep.sofa_score,sep.respiration,sep.coagulation,sep.liver,sep.cardiovascular,sep.cns,sep.renal,pt.dod\n",
        "FROM `physionet-data.mimiciv_derived.sepsis3` as sep\n",
        "INNER JOIN `physionet-data.mimiciv_hosp.patients` as pt\n",
        "ON sep.subject_id = pt.subject_id\n",
        "ORDER BY subject_id\n",
        "\"\"\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvkxNFGFqwEq"
      },
      "outputs": [],
      "source": [
        "##Retain the information columns for K-Means and DBSCAN\n",
        "\n",
        "pd.set_option('display.max_columns', None) ##This is only to show all column when printing a DataFrame\n",
        "\n",
        "patients = df['subject_id'] #Patients information will be separated\n",
        "dod = df['dod'] #Patients Date of Death information will be separated too.\n",
        "\n",
        "#Final Data set to work with:\n",
        "dataset = df.drop(['subject_id','dod'], axis = 1)\n",
        "\n",
        "## Check for missing values\n",
        "print(dataset.isnull().sum())\n",
        "\n",
        "data_km = dataset.copy()\n",
        "data_db = dataset.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caEjpTc0qwEq"
      },
      "source": [
        "## K-Means "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUks7tglqwEr"
      },
      "source": [
        "Main **Parameters** of the K-Means Function:\n",
        "\n",
        "**n_clusters** = the number of desired clusters. \n",
        "\n",
        "**init** = is the method for initialisation. (the default is ‘k-means++’, can also be ‘random’)\n",
        "\n",
        "**n_init** = how many times to run the k-means clustering algorithms independently with different random centroids to choose the final model (minimising the intra-cluster Sum of Squared Errors(SSE)).\n",
        "\n",
        "**max_iter** =  the maximum number of iterations for each single run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97gmGsaiqwEs"
      },
      "source": [
        "### How to chose the number of clusters? \n",
        "\n",
        "* We can do that cheching the intra-distance parameter (the smaller, the better).\n",
        "The atribute **_inertia** can measure it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LvlUYqKqwEs"
      },
      "outputs": [],
      "source": [
        "intradistance = [] \n",
        "for number_of_clusters in range(1, 11): \n",
        "    kmeans = KMeans(n_clusters = number_of_clusters, random_state = 0)\n",
        "    kmeans.fit(data_km) \n",
        "    intradistance.append(kmeans.inertia_)\n",
        "print(intradistance)\n",
        "\n",
        "## Elbow plot for decision:\n",
        "ks = [1,2,3,4,5,6,7,8,9,10]\n",
        "print(ks)\n",
        "plt.plot(ks, intradistance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ADwwO7qqwEt"
      },
      "source": [
        "### Using the Elbow plot to identify the optimal number of clusters. Can we can choose = 2 ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvZXoCisqwEt"
      },
      "outputs": [],
      "source": [
        "#Let's try:\n",
        "\n",
        "number_of_clusters = 2\n",
        "\n",
        "kmeans = KMeans(n_clusters = number_of_clusters, random_state = 0)\n",
        "kmeans.fit(data_km) \n",
        "    \n",
        "#Find which cluster each data-point belongs to\n",
        "clusters = kmeans.predict(data_km)\n",
        "\n",
        "#Add the cluster information as a new column to our DataFrame\n",
        "data_km[\"Cluster\"] = clusters\n",
        "\n",
        "print(data_km.groupby(['Cluster']).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrFd3Kw2qwEt"
      },
      "source": [
        "### Visualisation using PCA\n",
        "\n",
        "PCA is an algorithm that is used for **dimensionality reduction** - meaning, informally, that it can take in a DataFrame with many columns and return a DataFrame with a reduced number of columns that still retains much of the information from the columns of the original DataFrame. **The columns of the DataFrame produced from the PCA procedure are called Principal Components**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "VyRfwZOOqwEu"
      },
      "outputs": [],
      "source": [
        "#plotX is copy of the current DataFrame to be plotted:\n",
        "plotX = data_km\n",
        "\n",
        "#PCA with 2 principal components for a 2-D visualisation:\n",
        "pca_2d = PCA(n_components=2)\n",
        "\n",
        "#And this DataFrame contains 2 principal components that will aid us\n",
        "#in visualizing our clusters in \n",
        "PCs_2d = pd.DataFrame(pca_2d.fit_transform(plotX.drop([\"Cluster\"], axis=1)))\n",
        "PCs_2d.columns = [\"PC1_2d\", \"PC2_2d\"]\n",
        "\n",
        "\n",
        "#We concatenate these newly created DataFrames to plotX so that they can be used by plotX as columns.\n",
        "plotX = pd.concat([plotX,PCs_2d], axis=1, join='inner')\n",
        "\n",
        "#Now we divide plotX into 2 new DataFrames.\n",
        "#Each of these new DataFrames will hold all of the values contained in exacltly one of the clusters.\n",
        "\n",
        "cluster0 = plotX[plotX[\"Cluster\"] == 0]\n",
        "cluster1 = plotX[plotX[\"Cluster\"] == 1]\n",
        "\n",
        "\n",
        "## Calculate the mean value (Centroids) for each cluster to be seen in the Plot Figure:\n",
        "\n",
        "centr = [[cluster0[\"PC1_2d\"].mean(), cluster0[\"PC2_2d\"].mean()],\n",
        "         [cluster1[\"PC1_2d\"].mean(), cluster1[\"PC2_2d\"].mean()]]\n",
        "\n",
        "centr_x = [x for x,y in centr]\n",
        "centr_y = [y for x,y in centr]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LTyUhxaqwEu"
      },
      "outputs": [],
      "source": [
        "#This is needed so we can display plotly plots properly\n",
        "# init_notebook_mode(connected=True) #If you are running it locally as a jupyter notebook\n",
        "enable_plotly_in_cell() #If you are running it in Google Colab as a jupyter notebook\n",
        "\n",
        "#trace1 is for 'Cluster 0'\n",
        "trace1 = go.Scatter(\n",
        "                    x = cluster0[\"PC1_2d\"],\n",
        "                    y = cluster0[\"PC2_2d\"],\n",
        "                    mode = \"markers\",\n",
        "                    name = \"Cluster 0\",\n",
        "                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),\n",
        "                    text = None)\n",
        "\n",
        "#trace2 is for 'Cluster 1'\n",
        "trace2 = go.Scatter(\n",
        "                    x = cluster1[\"PC1_2d\"],\n",
        "                    y = cluster1[\"PC2_2d\"],\n",
        "                    mode = \"markers\",\n",
        "                    name = \"Cluster 1\",\n",
        "                    marker = dict(color = 'rgba(255, 128, 2, 0.8)'),\n",
        "                    text = None)\n",
        "\n",
        "\n",
        "# Include the centroids\n",
        "centroids = go.Scatter(x = centr_x,\n",
        "                       y = centr_y,\n",
        "                      mode = \"markers\",\n",
        "                      name = 'centroids',\n",
        "                      marker = dict(color = 'black'),\n",
        "                      text = None)\n",
        "\n",
        "data = [trace1, trace2, centroids]\n",
        "\n",
        "title = \"Visualizing Clusters in 2 Dimensions Using PCA\"\n",
        "\n",
        "layout = dict(title = title,\n",
        "              xaxis= dict(title= 'PC1',ticklen= 5,zeroline= False),\n",
        "              yaxis= dict(title= 'PC2',ticklen= 5,zeroline= False)\n",
        "             )\n",
        "\n",
        "# Plot the data:\n",
        "fig = dict(data = data, layout = layout)\n",
        "iplot(fig)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT0CYRMmm3jw"
      },
      "source": [
        "* We can use **BoxPlot to visually look for differences** between columns (For each Cluster):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2EaEvtFZUry"
      },
      "outputs": [],
      "source": [
        "\n",
        "boxplot = data_km.boxplot(column=['respiration','liver','cardiovascular'], by='Cluster')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF2hYQqhqwEv"
      },
      "source": [
        "## DBScan\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6A_OsPOqwEv"
      },
      "source": [
        "Main **Parameters** of the DBSCAN Function:\n",
        "\n",
        "eps = is the maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
        "(default is 0.5)\n",
        "\n",
        "min_samples = The number of samples (or total weight) in a neighborhood for a point to be considered as a core point.\n",
        "(default is 5)\n",
        "\n",
        "metric = The metric to use when calculating distance between instances in a feature array.\n",
        "(default is 'euclidean')\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9QrmD49qwEv"
      },
      "source": [
        "**Go ahead and play with the epsilon values and min sample values below to check the effects they have on the clusters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbDXniNJqwEw"
      },
      "outputs": [],
      "source": [
        "#Recap the data:\n",
        "\n",
        "print(data_db.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXnMl4KiqwEw"
      },
      "source": [
        "Calculate the average distance between each point in the data set and its 14 nearest neighbors (my selected MinPts value = 2* num columns)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3rsY6w5xqwEw"
      },
      "outputs": [],
      "source": [
        "neighbors = NearestNeighbors(n_neighbors=14)\n",
        "neighbors_fit = neighbors.fit(data_db)\n",
        "distances, indices = neighbors_fit.kneighbors(data_db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WqMylAXqwEw"
      },
      "source": [
        "Sort distance values by ascending value and plot: it will produce a k-distance elbow plot "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThFXoGDIqwEx"
      },
      "outputs": [],
      "source": [
        "distances = np.sort(distances, axis=0)\n",
        "distances = distances[:,1]\n",
        "plt.plot(distances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "XDVn5yT9qwEx"
      },
      "outputs": [],
      "source": [
        "# Compute DBSCAN\n",
        "db = DBSCAN(eps=1.25, min_samples=14) ## <- CHANGE THESE VALUES AND PLOT THE EFFECTS \n",
        "\n",
        "clustering_labels = db.fit_predict(data_db)\n",
        "data_db['labels'] = clustering_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoiRdWSJqwEx"
      },
      "source": [
        "### Evaluate Model Performance — Mean Silhouette Coefficient\n",
        "\n",
        "The Silhouette Coefficient is bounded between 1 and -1. The best value is 1, the worst is -1. A higher score indicates that the model has better defined, more dense clusters. Values close to 0 indicate overlapping clusters, while negative values usually indicate that data points have been assigned to the wrong clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHa9M0_DqwEx"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "metrics.silhouette_score(data_db, data_db['labels'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZCb4u1oqwEx"
      },
      "outputs": [],
      "source": [
        "labels = db.labels_\n",
        "print(set(labels))\n",
        "print(len(set(labels)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Tutorial04.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "47e86d731e077963188d400b641a1f5cee6401b89b8a1175acb1a082248e2517"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
