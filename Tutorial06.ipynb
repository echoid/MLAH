{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81G01biw-uRP"
      },
      "source": [
        "# Machine Learning Applications for Health (COMP90089_2022_SM2)\n",
        "# Tutorial 6: Natural Language Processing with Machine Learning Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnQlBha9G4qL"
      },
      "source": [
        "##NLP tasks employed here:\n",
        "* **Low-level task**: Split text into smaller units, create vectors (tokens).\n",
        "* **High-level task**: Make inferences based on the labelled text information.\n",
        "\n",
        "\n",
        "##Types of NLP Models:\n",
        "\n",
        "* **Rule-based**: Manually define and implement logic for processing text\n",
        "* **Statistical**: Machine Learning models trained on annotated data.\n",
        "\n",
        "> Here we are going to play with Statistical NLP model.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzRknuSsJ-TP"
      },
      "source": [
        "###Set up the environment: we will basically use **sklearn** library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "18E-G3-A1XJO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import linear_model, model_selection, preprocessing, metrics\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") \n",
        "\n",
        "#Display maximum \n",
        "pd.set_option('display.max_columns', None)\n",
        "np.set_printoptions(threshold=sys.maxsize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1-hF8RA_LHb"
      },
      "source": [
        "* ### **Data source:** Alec Chapman [GitHub](https://github.com/abchapman93/Melbourne_COMP90089_NLP) > Notebooks (05)\n",
        "\n",
        "\"The dataset in these examples is a set of MIMIC-II **radiology reports**. The annotations were created by University of Utah physician-scientist and **pneumonia** extraordinaire Dr. Barbara Jones. (A note which contains a positive or possible mention of a term referring to pneumonia might consider: Pneumonia; Pna; Opacity; Infiltrate; Consolidation)\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQoFg-Ft_-Fh"
      },
      "source": [
        "**Read the Data sets**: the training and testing set. \n",
        "\n",
        "Each set has **4 columns** as follows: \n",
        "* The document name: **'filename'**\n",
        "* The text: **'text'**\n",
        "* The annotator's document classification (whether or not the patient have pneumonia): **'document_classification'**\n",
        "* The baseline NLP system's document classification (this is the prediction result from another method. We will compare our model with it): **'baseline_document_classification'**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAYnPFYD2RLc"
      },
      "outputs": [],
      "source": [
        "##Download the data set from Alec GitHub\n",
        "url_train = 'https://raw.githubusercontent.com/abchapman93/Melbourne_COMP90089_NLP/main/melbourne_comp90089_nlp/data/pneumonia_data_train.json'\n",
        "url_test = 'https://raw.githubusercontent.com/abchapman93/Melbourne_COMP90089_NLP/main/melbourne_comp90089_nlp/data/pneumonia_data_test.json'\n",
        "\n",
        "#Read the data in the proper format\n",
        "df_train = pd.read_json(url_train)\n",
        "df_test = pd.read_json(url_test)\n",
        "\n",
        "#Check how the first rows looks like:\n",
        "print(df_train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_xmdQit7Go8"
      },
      "outputs": [],
      "source": [
        "#Check specifically the Text column which contains the raw information (radiology report notes)\n",
        "print(df_train['text'][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOvwh-Iu4pTP"
      },
      "source": [
        "### Verify how balanced is the annotaded data regarding Pneumonia:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooK-PlNPdTx-"
      },
      "outputs": [],
      "source": [
        "fig , ax = plt.subplots(figsize=(6,4))\n",
        "sns.countplot(x='document_classification', data=df_train)\n",
        "plt.title(\"Count of Pneumonia Annotations (0/1)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9uZbwUn8ZqX"
      },
      "source": [
        "##Text Analysis: Building Vectors (Tokens)\n",
        "\n",
        "Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols **cannot be fed directly to the algorithms themselves** as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
        "\n",
        "\n",
        "* The method from sklearn **CountVectorizer()** converts a collection of text documents to a matrix of token counts. Read more about it [here](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
        "\n",
        "* The method **fit.transform()** learns the vocabulary dictionary and return document-term matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xd4MSBp8ZM8"
      },
      "outputs": [],
      "source": [
        "#CountVectorizer: tokenizing strings and giving an integer id for each possible token. Also counting the occurrences of tokens in each document.\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "#Learn the vocabulary dictionary and return document-term matrix\n",
        "train_vectors = count_vectorizer.fit_transform(df_train[\"text\"])\n",
        "train_vectors #We will have 140 rows with 1358 unique words transformed into matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzegwG4fMtX7"
      },
      "outputs": [],
      "source": [
        "#Each term found by the analyzer during the fit is assigned a unique integer index corresponding to a column in the resulting matrix\n",
        "count_vectorizer.get_feature_names_out()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yERiYlj9Z66"
      },
      "source": [
        "Create Vectors (Tokens) for the testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "s5b7Wxnp9ZKq"
      },
      "outputs": [],
      "source": [
        "## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n",
        "# that the tokens in the train vectors are the only ones mapped to the test vectors - \n",
        "# i.e. that the train and test vectors use the same set of tokens.\n",
        "test_vectors = count_vectorizer.transform(df_test[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEypbPKbVBZW"
      },
      "source": [
        "###Machine Learning Model:\n",
        "\n",
        "* Random Forest [Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) applying [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to find the best set of parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brR02KcoDt2Y"
      },
      "outputs": [],
      "source": [
        "#Initialise the Random Forest Classifier\n",
        "rfc = RandomForestClassifier(random_state=0)\n",
        "\n",
        "#Check the Random Forest documentation to get the parameters. Let's set some possible values for each of them:\n",
        "param_grid = { \n",
        "    'n_estimators': [200, 500],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth' : [4,5,6,7,8],\n",
        "    'criterion' :['gini', 'entropy']\n",
        "}\n",
        "\n",
        "#Call the GridSearchCV function to analyse each possible combination of parameter, in this case: 2 * 3 * 5 * 2 = 60\n",
        "CV_rfc = GridSearchCV(estimator = rfc, param_grid = param_grid, cv= 5)\n",
        "CV_rfc.fit(train_vectors, df_train[\"document_classification\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNCwGuRo6Upf"
      },
      "source": [
        "* What is the **best set of parameters for the Random Forest** classifier using this data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqmHpecPgWvd"
      },
      "outputs": [],
      "source": [
        "CV_rfc.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIjFs1Wn6jhz"
      },
      "source": [
        "* Get the parameters and **call the classifier again**, now passing these information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WMyUHTebgurh"
      },
      "outputs": [],
      "source": [
        "rfc = RandomForestClassifier(criterion='gini',max_depth=5, max_features='auto', n_estimators=500, random_state=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfU9EOV5VPXr"
      },
      "outputs": [],
      "source": [
        "rfc.fit(train_vectors, df_train[\"document_classification\"])\n",
        "df_test[\"new_prediction_rforest\"] = rfc.predict(test_vectors)\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jETaw6P36--j"
      },
      "source": [
        "### Evaluate the model, how good is it? \n",
        "\n",
        "* The **sklearn.metrics** module implements several loss, score, and utility functions to measure classification performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJTtQl9LVr9Q"
      },
      "outputs": [],
      "source": [
        "#Accuracy classification score\n",
        "acc_rfc = float(round(metrics.accuracy_score(df_test[\"document_classification\"], df_test[\"new_prediction_rforest\"]),3))\n",
        "\n",
        "#Compute the balanced accuracy.\n",
        "bacc_rfc = float(round(metrics.balanced_accuracy_score(df_test[\"document_classification\"], df_test[\"new_prediction_rforest\"]),3))\n",
        "\n",
        "#Compute the Matthews correlation coefficient (MCC)\n",
        "mcc_rfc = float(round(metrics.matthews_corrcoef(df_test[\"document_classification\"], df_test[\"new_prediction_rforest\"]),3))\n",
        "\n",
        "#Compute the F1 score, also known as balanced F-score or F-measure.\n",
        "f1_rfc = float(round(metrics.f1_score(df_test[\"document_classification\"], df_test[\"new_prediction_rforest\"]),3))\n",
        "\n",
        "#Save results as a DataFrame:\n",
        "results_rfc = {'Accuracy' : [acc_rfc], 'Balanced Accuracy' : [bacc_rfc], 'MCC' : [mcc_rfc], 'F1-Score' : [f1_rfc]}\n",
        "rfc_results = pd.DataFrame.from_dict(data = results_rfc, orient='columns')\n",
        "print(rfc_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmcmDxEnWzuO"
      },
      "source": [
        "### **Comparison**: How is the baseline model?\n",
        "\n",
        "* Get the output predictions for the **baseline** model in the column df_test[\"baseline_document_classification\"] and check the metrics again for them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SK6gsSnWSjS"
      },
      "outputs": [],
      "source": [
        "#Accuracy classification score\n",
        "acc_base = float(round(metrics.accuracy_score(df_test[\"document_classification\"], df_test[\"baseline_document_classification\"]),3))\n",
        "\n",
        "#Compute the balanced accuracy.\n",
        "bacc_base = float(round(metrics.balanced_accuracy_score(df_test[\"document_classification\"], df_test[\"baseline_document_classification\"]),3))\n",
        "\n",
        "#Compute the Matthews correlation coefficient (MCC)\n",
        "mcc_base = float(round(metrics.matthews_corrcoef(df_test[\"document_classification\"], df_test[\"baseline_document_classification\"]),3))\n",
        "\n",
        "#Compute the F1 score, also known as balanced F-score or F-measure.\n",
        "f1_base = float(round(metrics.f1_score(df_test[\"document_classification\"], df_test[\"baseline_document_classification\"]),3))\n",
        "\n",
        "#Save results as a DataFrame:\n",
        "results_base = {'Accuracy' : [acc_base], 'Balanced Accuracy' : [bacc_base], 'MCC' : [mcc_base], 'F1-Score' : [f1_base]}\n",
        "base_results = pd.DataFrame.from_dict(data = results_base, orient='columns')\n",
        "print(base_results,\"\\n\")\n",
        "print(\"Compare with our result: \\n\",rfc_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5d1BK5F7obI"
      },
      "source": [
        "### What is your conclusion?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Sd3oOzD94pW"
      },
      "source": [
        "### **Bonus**: check how good a Linear Regression model would be with this data. \n",
        "\n",
        "* Linear Regression: Classifier using [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html) regression.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "un_7BpgL931A"
      },
      "outputs": [],
      "source": [
        "linear = linear_model.RidgeClassifier()\n",
        "\n",
        "scores = model_selection.cross_val_score(linear, train_vectors, df_train[\"document_classification\"], cv=3, scoring=\"f1\")\n",
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJKe-O3lCDG4"
      },
      "outputs": [],
      "source": [
        "linear.fit(train_vectors, df_train[\"document_classification\"])\n",
        "df_test[\"new_prediction_linear\"] = linear.predict(test_vectors)\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKP1AfMeDSY_"
      },
      "source": [
        "### Evaluate the model, how good is it? \n",
        "\n",
        "* The **sklearn.metrics** module implements several loss, score, and utility functions to measure classification performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO23UdVbDRFb"
      },
      "outputs": [],
      "source": [
        "#Accuracy classification score\n",
        "acc_linear = float(round(metrics.accuracy_score(df_test[\"document_classification\"], df_test[\"new_prediction_linear\"]),3))\n",
        "\n",
        "#Compute the balanced accuracy.\n",
        "bacc_linear = float(round(metrics.balanced_accuracy_score(df_test[\"document_classification\"], df_test[\"new_prediction_linear\"]),3))\n",
        "\n",
        "#Compute the Matthews correlation coefficient (MCC)\n",
        "mcc_linear = float(round(metrics.matthews_corrcoef(df_test[\"document_classification\"], df_test[\"new_prediction_linear\"]),3))\n",
        "\n",
        "#Compute the F1 score, also known as balanced F-score or F-measure.\n",
        "f1_linear = float(round(metrics.f1_score(df_test[\"document_classification\"], df_test[\"new_prediction_linear\"]),3))\n",
        "\n",
        "#Save results as a DataFrame:\n",
        "results = {'Accuracy' : [acc_linear], 'Balanced Accuracy' : [bacc_linear], 'MCC' : [mcc_linear], 'F1-Score' : [f1_linear]}\n",
        "linear_results = pd.DataFrame.from_dict(data = results, orient='columns')\n",
        "print(linear_results)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "47e86d731e077963188d400b641a1f5cee6401b89b8a1175acb1a082248e2517"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
